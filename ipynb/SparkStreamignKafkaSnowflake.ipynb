{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa50aa5-0c2b-42cf-960e-f5959815c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing integer: 88\n",
      "Pushing integer: 31\n",
      "Pushing integer: 84\n",
      "Pushing integer: 71\n",
      "Pushing integer: 61\n",
      "Pushing integer: 8\n",
      "Pushing integer: 80\n",
      "Pushing integer: 61\n",
      "Pushing integer: 25\n",
      "Pushing integer: 86\n"
     ]
    }
   ],
   "source": [
    "#Testing Spark data stream consumption and staging data to snowflake\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StockStreaming\").getOrCreate()\n",
    "\n",
    "# Kafka configuration\n",
    "kafka_conf = {\n",
    "    'bootstrap.servers': '127.0.0.1:9092',  # Kafka broker address\n",
    "    'group.id': 'stock-data-consumer',  # Consumer group ID\n",
    "    'auto.offset.reset': 'earliest'  # Start from the earliest offset\n",
    "}\n",
    "\n",
    "# Define the Kafka topic to consume from\n",
    "kafka_topic = 'stock-streaming-topic'\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = Consumer(kafka_conf)\n",
    "consumer.subscribe([kafka_topic])\n",
    "\n",
    "# Define the schema for the incoming JSON messages (customize as per your data)\n",
    "schema = StructType([\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Consume and process Kafka messages\n",
    "while True:\n",
    "    message = consumer.poll(1.0)\n",
    "\n",
    "    if message is None:\n",
    "        continue\n",
    "\n",
    "    if message.error():\n",
    "        if message.error().code() == KafkaError._PARTITION_EOF:\n",
    "            print(f'Reached end of partition {message.partition()}.')\n",
    "        else:\n",
    "            print(f'Error while consuming message: {message.error()}')\n",
    "    else:\n",
    "        # Decode and parse the JSON message\n",
    "        msg_value = message.value().decode('utf-8')\n",
    "        msg_df = spark.read.json(spark.sparkContext.parallelize([msg_value]), schema=schema)\n",
    "\n",
    "        # Perform any necessary data transformations or processing here\n",
    "        # For example, you can aggregate data, perform calculations, etc.\n",
    "        # For this example, we'll simply display the received data\n",
    "        msg_df.show()\n",
    "\n",
    "        # Save the DataFrame to Snowflake (customize Snowflake configuration)\n",
    "        msg_df.write \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .option(\"sfURL\", \"YOUR_SNOWFLAKE_URL\") \\\n",
    "            .option(\"sfDatabase\", \"YOUR_DATABASE\") \\\n",
    "            .option(\"sfWarehouse\", \"YOUR_WAREHOUSE\") \\\n",
    "            .option(\"sfRole\", \"YOUR_ROLE\") \\\n",
    "            .option(\"sfSchema\", \"YOUR_SCHEMA\") \\\n",
    "            .option(\"sfWarehouse\", \"YOUR_WAREHOUSE\") \\\n",
    "            .option(\"sfDatabase\", \"YOUR_DATABASE\") \\\n",
    "            .option(\"sfSchema\", \"YOUR_SCHEMA\") \\\n",
    "            .option(\"sfRole\", \"YOUR_ROLE\") \\\n",
    "            .option(\"sfDatabase\", \"YOUR_DATABASE\") \\\n",
    "            .option(\"sfWarehouse\", \"YOUR_WAREHOUSE\") \\\n",
    "            .option(\"sfSchema\", \"YOUR_SCHEMA\") \\\n",
    "            .option(\"sfRole\", \"YOUR_ROLE\") \\\n",
    "            .option(\"sfWarehouse\", \"YOUR_WAREHOUSE\") \\\n",
    "            .option(\"sfSchema\", \"YOUR_SCHEMA\") \\\n",
    "            .option(\"sfRole\", \"YOUR_ROLE\") \\\n",
    "            .option(\"sfDatabase\", \"YOUR_DATABASE\") \\\n",
    "            .option(\"sfWarehouse\", \"YOUR_WAREHOUSE\") \\\n",
    "            .option(\"sfSchema\", \"YOUR_SCHEMA\") \\\n",
    "            .option(\"sfRole\", \"YOUR_ROLE\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "# Stop Spark session (this won't execute in an infinite loop)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98371375-7327-4dd6-b835-e9abf3cb42b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing consume integers stream from Kafka. working\n",
    "from confluent_kafka import Consumer, KafkaError, Producer\n",
    "\n",
    "def get_kafka_topics(broker):\n",
    "    \"\"\"\n",
    "    Fetches and prints the list of topics from the Kafka broker.\n",
    "    Returns an empty list if not connected.\n",
    "    \"\"\"\n",
    "    producer_conf = {\n",
    "        'bootstrap.servers': broker\n",
    "    }\n",
    "    producer = Producer(producer_conf)\n",
    "\n",
    "    # Fetch metadata\n",
    "    cluster_metadata = producer.list_topics(timeout=10)\n",
    "    \n",
    "    # If no broker metadata is returned, it means we're not connected\n",
    "    if not cluster_metadata.brokers:\n",
    "        print(\"Failed to connect to Kafka!\")\n",
    "        return []\n",
    "    \n",
    "    topic_names = list(cluster_metadata.topics.keys())\n",
    "    return topic_names\n",
    "\n",
    "def consume_messages(topic, num_messages):\n",
    "    conf = {\n",
    "        'bootstrap.servers': '127.0.0.1:9092', \n",
    "        'group.id': 'my_group',\n",
    "        'auto.offset.reset': 'earliest'\n",
    "    }\n",
    "    \n",
    "    topics = get_kafka_topics(conf['bootstrap.servers'])\n",
    "    if not topics:\n",
    "        print(\"Couldn't fetch list of topics.\")\n",
    "        return []\n",
    "    print(f\"List of topics in Kafka: {', '.join(topics)}\")\n",
    "\n",
    "    if topic not in topics:\n",
    "        print(f\"Topic '{topic}' not found in Kafka.\")\n",
    "        return []\n",
    "\n",
    "    consumer = Consumer(conf)\n",
    "    consumer.subscribe([topic])\n",
    "\n",
    "    messages = []\n",
    "    for _ in range(num_messages):\n",
    "        message = consumer.poll(1.0)\n",
    "        if message is None:\n",
    "            continue\n",
    "        if message.error():\n",
    "            if message.error().code() == KafkaError._PARTITION_EOF:\n",
    "                # End of partition event - not an error.\n",
    "                print(f'Reached end of partition {message.partition()}.')\n",
    "            else:\n",
    "                print(f'Error while consuming message: {message.error()}')\n",
    "        else:\n",
    "            # Proper message\n",
    "            messages.append(message.value().decode('utf-8'))\n",
    "    \n",
    "    consumer.close()\n",
    "    return messages\n",
    "\n",
    "# Consume messages from the '2integers-stream' topic\n",
    "num_messages_to_fetch = 100  # You can change this to the number of messages you wish to fetch\n",
    "messages = consume_messages('demo-messages', num_messages_to_fetch)\n",
    "\n",
    "# Print out the content of the messages\n",
    "for msg in messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef6035-9c62-4db9-a488-cad9b45271ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
